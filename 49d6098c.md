# Code Review: Slide Deck Generator with Gemini API

## Overview
Your slide generation module uses the Google GenAI SDK to create educational presentations with AI-generated content, speaker notes, and images. Several issues affect reliability and best practices around JSON parsing, image decoding, citation handling, and prompt consistency.

---

## Critical Issues & Recommended Fixes

### 1. Unreliable JSON Parsing for Slides
**Issue:**  
You ask the model to return "valid JSON" in the prompt, then strip code fences and parse with `JSON.parse()`. The model may add trailing commentary, emit slightly malformed JSON, or wrap output in extra markdown—all causing silent failures or exceptions that force error handling at runtime.

**Fix:**  
Use Gemini's **Structured Outputs** feature to enforce JSON at the API level:
```typescript
const slidesSchema = {
  type: "array",
  items: {
    type: "object",
    properties: {
      title: { type: "string" },
      content: { 
        type: "array", 
        items: { type: "string" }
      },
      layout: { 
        type: "string", 
        enum: ["Title Slide", "Content"] 
      },
      imagePrompt: { type: "string" },
      speakerNotes: { type: "string" },
    },
    required: ["title", "content", "layout", "imagePrompt", "speakerNotes"],
  },
};

const response = await ai.models.generateContent({
  model: "gemini-2.5-pro",
  contents: prompt,
  config: {
    temperature: temperature,
    tools: tools.length > 0 ? tools : undefined,
    responseMimeType: "application/json",
    responseSchema: slidesSchema,
  },
});

const slides: Slide[] = JSON.parse(response.text);
```

**Benefits:**
- Guaranteed syntactically valid JSON (no markdown cleanup needed)
- Schema validation at generation time reduces semantic errors
- Cleaner error handling

---

### 2. Weak Output Validation
**Issue:**  
Even with a prompt rule stating "Exactly N bullets," the model can return wrong counts. You don't validate `slides.length`, `content.length`, or other schema constraints at runtime.

**Fix:**  
Add explicit validation after parsing:
```typescript
const slides: Slide[] = JSON.parse(response.text);

// Validate structure
if (slides.length !== totalSlides) {
  throw new Error(`Expected ${totalSlides} slides, got ${slides.length}`);
}

slides.forEach((slide, idx) => {
  if (slide.layout === "Content" && slide.content.length !== bulletsPerSlide) {
    throw new Error(
      `Slide ${idx + 1}: expected ${bulletsPerSlide} bullets, got ${slide.content.length}`
    );
  }
  if (!slide.speakerNotes) {
    slide.speakerNotes = "Speaker notes were not generated for this slide.";
  }
});

return { slides, inputTokens: response.usageMetadata?.promptTokenCount || 0, outputTokens: response.usageMetadata?.candidatesTokenCount || 0 };
```

**Benefits:**
- Fail fast with clear error messages
- Prevent downstream rendering errors
- Aid debugging

---

### 3. Web Search Citations Handled Unreliably
**Issue:**  
When `useWebSearch` is enabled, you instruct the model to end `speakerNotes` with a "Sources:" section listing URLs. However:
- The Gemini API already returns structured grounding metadata (`groundingMetadata`) when Google Search is enabled
- Relying on the model to embed URLs in text is redundant, inconsistent, and hard to parse
- This approach loses the semantic link between content claims and their sources

**Fix:**  
Use Gemini's built-in grounding metadata instead:

1. **Add a `sources` field to your schema:**
```typescript
const slidesSchema = {
  type: "array",
  items: {
    type: "object",
    properties: {
      title: { type: "string" },
      content: { type: "array", items: { type: "string" } },
      layout: { type: "string", enum: ["Title Slide", "Content"] },
      imagePrompt: { type: "string" },
      speakerNotes: { type: "string" },
      sources: { type: "array", items: { type: "string" } }, // NEW
    },
    required: ["title", "content", "layout", "imagePrompt", "speakerNotes"],
  },
};
```

2. **Extract grounding metadata and attach to slides:**
```typescript
const response = await ai.models.generateContent({
  model: "gemini-2.5-pro",
  contents: prompt,
  config: {
    temperature: temperature,
    tools: tools.length > 0 ? tools : undefined,
    responseMimeType: "application/json",
    responseSchema: slidesSchema,
  },
});

const slides: Slide[] = JSON.parse(response.text);

// Extract grounding metadata if web search was used
if (useWebSearch && response.candidates?.[0]?.groundingMetadata) {
  const groundingMetadata = response.candidates[0].groundingMetadata;
  // Store citations separately or attach to response
  // (You can map groundingMetadata.groundingChunks to slide indices as needed)
}
```

3. **Update your prompt** to stop asking for embedded "Sources:" sections:
```typescript
// Remove this line from the prompt:
// "speakerNotes": "string (Start with script. End with a 'Sources:' section listing URLs if Web Search was used)"
// Replace with:
// "speakerNotes": "string (Conversational script explaining the slide content)"
```

**Benefits:**
- Leverage structured grounding data from the API
- Cleaner separation of concerns (content ≠ citations)
- Easier to render and validate citations in your UI
- Consistent with Gemini's design for grounded generation

---

### 4. Image Decoding Bug in Node.js Environment
**Issue:**  
`atob()` is not reliably available in Node.js (it's a browser API). Your base64-to-byte conversion will fail or produce incorrect results on server-side environments like Vercel or Firebase.

**Fix:**  
Use `Buffer.from()` instead:
```typescript
const parts = response.candidates?.[0]?.content?.parts ?? [];

for (const part of parts) {
  const inlineData = (part as any).inlineData ?? (part as any).inline_data;
  if (inlineData?.data) {
    const mimeType = inlineData.mimeType ?? inlineData.mime_type ?? "image/png";
    
    // Replace atob + Uint8Array with Buffer.from (Node-safe)
    const bytes = Buffer.from(inlineData.data, "base64");
    return new Blob([bytes], { type: mimeType });
  }
}

throw new Error("No image data found in response");
```

**Benefits:**
- Works reliably in Node.js/server environments
- Simpler, more maintainable
- No manual byte conversion needed

---

### 5. Image Response Traversal Path
**Issue:**  
You check `(response as any).parts` first, but the documented/correct path for the Google GenAI SDK is `response.candidates[0].content.parts`. The `.parts` fallback may work in some SDK versions but is not guaranteed.

**Fix:**  
Prioritize the correct documented path and keep a fallback:
```typescript
const parts = response.candidates?.[0]?.content?.parts ?? [];

if (!parts || parts.length === 0) {
  // Only fallback if the documented path yields nothing
  const fallbackParts = (response as any).parts;
  if (fallbackParts && fallbackParts.length > 0) {
    // Process fallback
  } else {
    console.error("Response structure:", JSON.stringify(response, null, 2));
    throw new Error("No image data found in response");
  }
}

for (const part of parts) {
  const inlineData = (part as any).inlineData ?? (part as any).inline_data;
  if (inlineData?.data) {
    const mimeType = inlineData.mimeType ?? inlineData.mime_type ?? "image/png";
    const bytes = Buffer.from(inlineData.data, "base64");
    return new Blob([bytes], { type: mimeType });
  }
}
```

**Benefits:**
- Adheres to documented SDK behavior
- Reduces version-specific breakage
- Clearer error messages when structure is truly invalid

---

### 6. Prompt Constraint Mismatch: "No Markdown" vs. Markdown-Heavy Prompts
**Issue:**  
Your prompts contain extensive Markdown formatting (`**...**`, `-` lists, etc.), but you tell the model "**No Markdown:** Bullet points must be plain strings. NO bold (**), italics (*), or bullet characters (-) in the string itself."

This inconsistency can cause the model to:
- Interpret the examples as acceptable and apply them to output fields
- Embed Markdown in `content` or `speakerNotes` strings
- Undermine the clarity of your constraints

**Fix:**  
Convert prompt sections to plain-text formatting:
```typescript
// BEFORE (Markdown-heavy):
prompt += `
  **Presentation Context:**
  - Topic: "${topic}"
  - Subject: ${subject}
`;

// AFTER (Plain text):
prompt += `
  PRESENTATION CONTEXT
  Topic: "${topic}"
  Subject: ${subject}
`;
```

Apply this consistently across all prompt sections, and replace the `IMAGE_STYLE_GUIDE` constant:
```typescript
// BEFORE:
const IMAGE_STYLE_GUIDE = `
**Visual Style Guidelines:**
- **Art Style:** Flat vector-style educational illustration. Supplementary visual aid. Professional, clean lines.
- **Background:** Clean, solid, or white background. No scenic backgrounds or visual clutter.
- **Color & Contrast:** High contrast, distinct colors optimized for classroom projection.
- **No Text:** The image must be completely text-free. No labels, no words, no letters, no numbers.
`;

// AFTER:
const IMAGE_STYLE_GUIDE = `
VISUAL STYLE GUIDELINES

Art Style: Flat vector-style educational illustration. Supplementary visual aid. Professional, clean lines.
Background: Clean, solid, or white background. No scenic backgrounds or visual clutter.
Color and Contrast: High contrast, distinct colors optimized for classroom projection.
No Text: The image must be completely text-free. No labels, no words, no letters, no numbers.
`;
```

**Benefits:**
- Clearer, less ambiguous prompts
- Reduces model confusion about acceptable output formats
- Strengthens "no markdown in bullets" constraint

---

### 7. Token Usage Field Assumptions
**Issue:**  
You assume `response.usageMetadata?.candidatesTokenCount` represents "output tokens," but this field may not be present or may be named differently across SDK versions and models. Assumptions can cause silent failures or incorrect telemetry.

**Fix:**  
Handle usage safely and defensively:
```typescript
const inputTokens = response.usageMetadata?.promptTokenCount ?? 0;
const outputTokens = response.usageMetadata?.candidatesTokenCount ?? 0;

// Optional: log actual fields returned for debugging
if (!response.usageMetadata) {
  console.warn("No usage metadata returned from API");
}

return {
  slides,
  inputTokens,
  outputTokens,
};
```

Or, if you need stricter telemetry, map all available usage fields:
```typescript
return {
  slides,
  usageMetadata: {
    promptTokens: response.usageMetadata?.promptTokenCount ?? 0,
    outputTokens: response.usageMetadata?.candidatesTokenCount ?? 0,
    // Store any other fields present
  },
};
```

**Benefits:**
- Graceful degradation if fields are missing
- Clear telemetry even when usage data varies by model
- Easier to adapt if Google's API evolves

---

### 8. Consistency with Google Search Grounding Tool
**Issue:**  
Your use of `tools.push({ googleSearch: {} })` is correct and aligns with Gemini's grounding feature, but the current prompt asks the model to redundantly embed URLs in text. This disconnects the tool's intent from the actual output.

**Fix:**  
Keep the tool setup (it's correct) but rely on structured grounding metadata instead:

1. **Keep the tool enablement as-is:**
```typescript
const tools: any[] = [];
if (useWebSearch && !sourceMaterial) {
  tools.push({ googleSearch: {} });
}

const config: any = {
  temperature: temperature,
  tools: tools.length > 0 ? tools : undefined,
  responseMimeType: "application/json",
  responseSchema: slidesSchema,
};
```

2. **Remove the prompt instruction to embed URLs:**
```typescript
// OLD:
// "speakerNotes": "string (Start with script. End with a 'Sources:' section listing URLs if Web Search was used)"

// NEW:
// "speakerNotes": "string (Conversational script explaining the slide content)"
```

3. **Extract citations from `groundingMetadata` post-generation** (see Issue #3 above).

**Benefits:**
- Tool and output handling are aligned
- API design intent is respected
- Cleaner separation of data and metadata

---

## Summary of Changes

| Issue | Fix | Priority |
|-------|-----|----------|
| Unreliable JSON parsing | Use Structured Outputs (`responseMimeType` + `responseSchema`) | **High** |
| Weak validation | Validate slide counts and bullet counts at runtime | **High** |
| Citation handling | Extract `groundingMetadata` instead of parsing text | **High** |
| Image decoding | Replace `atob()` with `Buffer.from()` | **High** |
| Image response path | Prioritize `candidates[0].content.parts` | **Medium** |
| Prompt formatting | Remove Markdown from prompts | **Medium** |
| Token usage assumptions | Handle missing/varying `usageMetadata` fields safely | **Low** |
| Grounding tool alignment | Remove manual URL embedding from prompts | **Medium** |

---

## Implementation Priority

1. **Phase 1 (High Priority):**
   - Add Structured Outputs to `generateSlidesFromDocument()`
   - Fix image decoding in `generateImage()` (Buffer.from)
   - Add runtime validation for slides and bullets

2. **Phase 2 (Medium Priority):**
   - Extract grounding metadata for citations
   - Fix image response traversal path
   - Refactor prompt formatting (remove Markdown)

3. **Phase 3 (Low Priority):**
   - Harden token usage telemetry
   - Add comprehensive logging/debugging
   - Add retry logic for validation failures

---

## Testing Recommendations

- Test with web search enabled and disabled to verify citation handling
- Test with images of various sizes and MIME types
- Test on both Node.js (server) and client environments to catch platform-specific issues
- Test with different `bulletsPerSlide` values and `numSlides` to verify validation
- Monitor token usage across multiple generations to validate telemetry
